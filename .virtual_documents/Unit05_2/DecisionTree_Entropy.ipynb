


import numpy as np 
from math import log
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline


#only 2 class
def E(a,b):
    if a == 0 or b ==0:
        return 0
    probs_a = a / (a+b)
    probs_b = b / (a+b)
    #Compute entropy
    ent = -(probs_a * log(probs_a,2) + probs_b * log(probs_b,2))
    
    return ent

def entropy(labels):
    n_labels = len(labels)

    if n_labels <= 1:
        return 0

    value, counts = np.unique(labels, return_counts=True)
    print(value, counts)
    probs = counts / n_labels

    ent = 0.
    # Compute entropy
    for i in probs:
        ent -= i * log(i, 2)

    return ent


S=E(9,5)
S





S-(5/14*E(3,2)+4/14*E(4,0)+5/14*E(2,3))





S-(4/14*E(2,2)+6/14*E(4,2)+4/14*E(3,1))





S- (7/14*E(3,4)+7/14*E(6,1))





S-(8/14*E(6,2)+6/14*E(3,3))





E(3,2)


# temp gain
E(3,2) - (2/5*E(2,0)+2/5*E(1,1)+1/5*E(1,0))


# humility gain
E(3,2) - (3/5*E(3,0)+2/5*E(2,0))


# windy gain
E(3,2) - (3/5*E(2,1)+2/5*E(1,1))


def entropy(labels):
    n_labels = len(labels)

    if n_labels <= 1:
        return 0

    value,counts = np.unique(labels, return_counts=True)
    print(value,counts)
    probs = counts / n_labels

    ent = 0.
    # Compute entropy
    for i in probs:
        ent -= i * log(i,2)

    return ent


labels=['yes','yes','yes']
print(entropy(labels))

labels=['yes','no','yes','no']
print(entropy(labels))

labels=['high','low','high','high','low','normal','normal','normal']
print(entropy(labels))

labels=['high','high','high','high','high','normal','normal','normal']
print(entropy(labels))





from scipy.stats import entropy


entropy([1/2, 1/2], base=2)


entropy([5/8, 3/8], base=2)


0.97 - (2/5*entropy([2,0], base=2)+2/5*entropy([1,1], base=2)+1/5*entropy([1,0], base=2))




